{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f435bc0",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbd8195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:32.262580Z",
     "start_time": "2022-12-04T13:37:30.061476Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d543de",
   "metadata": {},
   "source": [
    "## seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2690afe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:32.333872Z",
     "start_time": "2022-12-04T13:37:32.264564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94a87e",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "Hugging Face Hub에 존재하는 Pretrained Tokenizer 불러오기\n",
    "\n",
    "[URL] 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4694db48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:37.999165Z",
     "start_time": "2022-12-04T13:37:32.335193Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'klue/roberta-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858fa2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:38.008764Z",
     "start_time": "2022-12-04T13:37:38.001359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-small', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97254e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:38.019149Z",
     "start_time": "2022-12-04T13:37:38.009937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "1 32000\n"
     ]
    }
   ],
   "source": [
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': ['[URL]']\n",
    "}\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "print(num_added_toks, tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5838ee61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:38.033882Z",
     "start_time": "2022-12-04T13:37:38.020689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-small', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[URL]']})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cd9f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:38.046972Z",
     "start_time": "2022-12-04T13:37:38.035611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['야', '오버', '##워', '##치', '(', ')', '2', '해', '##봄', '?', '지금', '이벤트', '##도', '하', '##는데', '같이', '하자', '!', '!', '[URL]']\n",
      "{'input_ids': [0, 1396, 10737, 2667, 2225, 12, 13, 22, 1897, 3064, 35, 3660, 5028, 2119, 1889, 13964, 3848, 20651, 5, 5, 32000, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 야 오버워치 ( ) 2 해봄? 지금 이벤트도 하는데 같이 하자!! [URL] [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"야 오버워치()2 해봄? 지금 이벤트도 하는데 같이 하자!! [URL]\"))\n",
    "print(tokenizer(\"야 오버워치()2 해봄? 지금 이벤트도 하는데 같이 하자!! [URL]\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"야 오버워치()2 해봄? 지금 이벤트도 하는데 같이 하자!! [URL]\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b52c5a",
   "metadata": {},
   "source": [
    "## Model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04360207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:40.199523Z",
     "start_time": "2022-12-04T13:37:38.049469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76bb7ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:40.447642Z",
     "start_time": "2022-12-04T13:37:40.200631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token embedding resize\n",
    "\n",
    "model.resize_token_embeddings(tokenizer.vocab_size + num_added_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dde2d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:40.454669Z",
     "start_time": "2022-12-04T13:37:40.450267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32001, 768)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1c073",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107f0b2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:40.631474Z",
     "start_time": "2022-12-04T13:37:40.456615Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"/home/egg2018037024/Interlink_Project/Preprocess_Data/train.csv\", encoding='cp949', index_col=0)\n",
    "valid_dataset = pd.read_csv(\"/home/egg2018037024/Interlink_Project/Preprocess_Data/valid.csv\", encoding='cp949', index_col=0)\n",
    "test_dataset = pd.read_csv(\"/home/egg2018037024/Interlink_Project/Preprocess_Data/test.csv\", encoding='cp949', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4115d27b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:48.402727Z",
     "start_time": "2022-12-04T13:37:40.634393Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['Sentence']),\n",
    "    return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "    max_length=256, \n",
    "    padding=True,  # batch 중 가장 긴 시퀀스를 기준으로 pad 채움.\n",
    "    truncation=True,  # max_length 넘어가면 버림\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_valid = tokenizer(\n",
    "    list(valid_dataset['Sentence']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_test = tokenizer(\n",
    "    list(test_dataset['Sentence']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "724c6e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:48.408593Z",
     "start_time": "2022-12-04T13:37:48.404133Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  3788,  1536,  2359, 13964,  4035,  2052,  1415,  2203,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1])\n",
      "[CLS] 설명 적었는데 사진이 없네 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train['input_ids'][970])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][970]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726795a",
   "metadata": {},
   "source": [
    "## Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb78e308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:48.425693Z",
     "start_time": "2022-12-04T13:37:48.411340Z"
    }
   },
   "outputs": [],
   "source": [
    "class klue_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, label):  # 전처리된 데이터 셋이 들어옴\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # gradient 계산에 영향을 주지 않게 clone().detach() 실행\n",
    "        \n",
    "        item = {key: val[idx].clone().detach() for key, val in self.dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):  # 샘플 수\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4171c43c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:48.452286Z",
     "start_time": "2022-12-04T13:37:48.428583Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87600\n",
      "10950\n",
      "{'input_ids': tensor([    0,  3788,  1536,  2359, 13964,  4035,  2052,  1415,  2203,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(0)}\n",
      "[CLS] 설명 적었는데 사진이 없네 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "train_klue_dataset = klue_Dataset(tokenized_train, train_dataset['Label'])\n",
    "valid_klue_dataset = klue_Dataset(tokenized_valid, valid_dataset['Label'])\n",
    "test_klue_dataset = klue_Dataset(tokenized_test, test_dataset['Label'])\n",
    "\n",
    "print(train_klue_dataset.__len__())\n",
    "print(valid_klue_dataset.__len__())\n",
    "print(train_klue_dataset.__getitem__(970))\n",
    "print(tokenizer.decode(train_klue_dataset.__getitem__(970)['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091bfa11",
   "metadata": {},
   "source": [
    "## 모델 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa8b3dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:48.460381Z",
     "start_time": "2022-12-04T13:37:48.454595Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf6645",
   "metadata": {},
   "source": [
    "## Trainer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc90e6e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T13:37:51.312718Z",
     "start_time": "2022-12-04T13:37:48.462684Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "training_ars = TrainingArguments(\n",
    "    output_dir='./klue_roberta_small_result',\n",
    "    num_train_epochs=5,\n",
    "    max_steps=10000,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    save_total_limit=5,\n",
    "   # save_strategy = \"no\",\n",
    "   # save_steps=10000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "   # load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_klue_dataset,\n",
    "    eval_dataset=valid_klue_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786585a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T04:42:08.202595Z",
     "start_time": "2022-12-04T13:37:51.315546Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 87600\n",
      "  Num Epochs = 292\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100000\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100000/100000 15:04:13, Epoch 291/292]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.016935</td>\n",
       "      <td>0.994338</td>\n",
       "      <td>0.985998</td>\n",
       "      <td>0.975424</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>0.995068</td>\n",
       "      <td>0.987535</td>\n",
       "      <td>0.998599</td>\n",
       "      <td>0.976712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.009005</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993831</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994045</td>\n",
       "      <td>0.997243</td>\n",
       "      <td>0.990868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.996438</td>\n",
       "      <td>0.991154</td>\n",
       "      <td>0.984678</td>\n",
       "      <td>0.997717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994275</td>\n",
       "      <td>0.997244</td>\n",
       "      <td>0.991324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994523</td>\n",
       "      <td>0.994069</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.995872</td>\n",
       "      <td>0.991324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.025164</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992195</td>\n",
       "      <td>0.997692</td>\n",
       "      <td>0.986758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.012105</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>0.994064</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993169</td>\n",
       "      <td>0.990463</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994067</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.995661</td>\n",
       "      <td>0.995889</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.024956</td>\n",
       "      <td>0.995616</td>\n",
       "      <td>0.989135</td>\n",
       "      <td>0.980700</td>\n",
       "      <td>0.997717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994048</td>\n",
       "      <td>0.996786</td>\n",
       "      <td>0.991324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>0.998174</td>\n",
       "      <td>0.995425</td>\n",
       "      <td>0.997250</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.010751</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993173</td>\n",
       "      <td>0.990018</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>0.997251</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.011345</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0.991364</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.016535</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994080</td>\n",
       "      <td>0.991371</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993826</td>\n",
       "      <td>0.995419</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.013812</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993390</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.015689</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994291</td>\n",
       "      <td>0.994518</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.015941</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.993619</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.994731</td>\n",
       "      <td>0.998161</td>\n",
       "      <td>0.991324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.020587</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993831</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993393</td>\n",
       "      <td>0.991360</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992898</td>\n",
       "      <td>0.996322</td>\n",
       "      <td>0.989498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992485</td>\n",
       "      <td>0.990005</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.013661</td>\n",
       "      <td>0.998082</td>\n",
       "      <td>0.995198</td>\n",
       "      <td>0.996793</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993575</td>\n",
       "      <td>0.998616</td>\n",
       "      <td>0.988584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993831</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994075</td>\n",
       "      <td>0.992266</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.009980</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.995649</td>\n",
       "      <td>0.998622</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.017192</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0.991364</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.012615</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994518</td>\n",
       "      <td>0.994973</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.014581</td>\n",
       "      <td>0.997991</td>\n",
       "      <td>0.994970</td>\n",
       "      <td>0.996337</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994053</td>\n",
       "      <td>0.995875</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994278</td>\n",
       "      <td>0.996788</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.014698</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993831</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.996164</td>\n",
       "      <td>0.990472</td>\n",
       "      <td>0.984220</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993601</td>\n",
       "      <td>0.994511</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.016782</td>\n",
       "      <td>0.997991</td>\n",
       "      <td>0.994979</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993610</td>\n",
       "      <td>0.993157</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994299</td>\n",
       "      <td>0.993166</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.994752</td>\n",
       "      <td>0.994072</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.021496</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994523</td>\n",
       "      <td>0.994069</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994523</td>\n",
       "      <td>0.994069</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993840</td>\n",
       "      <td>0.993160</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993616</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.013401</td>\n",
       "      <td>0.998174</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.995887</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993387</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.026091</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992667</td>\n",
       "      <td>0.996320</td>\n",
       "      <td>0.989041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.018938</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992908</td>\n",
       "      <td>0.994956</td>\n",
       "      <td>0.990868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.032782</td>\n",
       "      <td>0.996164</td>\n",
       "      <td>0.990323</td>\n",
       "      <td>0.999535</td>\n",
       "      <td>0.981279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993365</td>\n",
       "      <td>0.995415</td>\n",
       "      <td>0.991324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.023025</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.990455</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.020843</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993347</td>\n",
       "      <td>0.998156</td>\n",
       "      <td>0.988584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.994738</td>\n",
       "      <td>0.996790</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994042</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.990411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>0.990247</td>\n",
       "      <td>0.983776</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.019726</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991811</td>\n",
       "      <td>0.988214</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.994748</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.996438</td>\n",
       "      <td>0.991134</td>\n",
       "      <td>0.986872</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.015726</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994056</td>\n",
       "      <td>0.995421</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.014958</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.994755</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.022094</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990697</td>\n",
       "      <td>0.984664</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.998082</td>\n",
       "      <td>0.995202</td>\n",
       "      <td>0.995885</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.998174</td>\n",
       "      <td>0.995423</td>\n",
       "      <td>0.997706</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.010636</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994301</td>\n",
       "      <td>0.992717</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.993610</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.018436</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0.991364</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994037</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>0.989498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993616</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993619</td>\n",
       "      <td>0.991811</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.019460</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992485</td>\n",
       "      <td>0.990005</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.023012</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992714</td>\n",
       "      <td>0.990009</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.021676</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994061</td>\n",
       "      <td>0.994516</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993135</td>\n",
       "      <td>0.995413</td>\n",
       "      <td>0.990868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993166</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.019601</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992216</td>\n",
       "      <td>0.994949</td>\n",
       "      <td>0.989498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993157</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994296</td>\n",
       "      <td>0.993616</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.016327</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993840</td>\n",
       "      <td>0.993160</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992457</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.991324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.017916</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993359</td>\n",
       "      <td>0.996325</td>\n",
       "      <td>0.990411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.022378</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992269</td>\n",
       "      <td>0.988225</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.048434</td>\n",
       "      <td>0.994247</td>\n",
       "      <td>0.985782</td>\n",
       "      <td>0.974565</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.034112</td>\n",
       "      <td>0.994247</td>\n",
       "      <td>0.985782</td>\n",
       "      <td>0.974565</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.024009</td>\n",
       "      <td>0.993881</td>\n",
       "      <td>0.984466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.015498</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993823</td>\n",
       "      <td>0.995873</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.021642</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992491</td>\n",
       "      <td>0.989116</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993843</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993393</td>\n",
       "      <td>0.991360</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.019961</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.989555</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.018966</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993390</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.017235</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994523</td>\n",
       "      <td>0.994069</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.017979</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994523</td>\n",
       "      <td>0.994069</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993390</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992937</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.018999</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993616</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.017459</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994069</td>\n",
       "      <td>0.993163</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>0.996347</td>\n",
       "      <td>0.990884</td>\n",
       "      <td>0.989081</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.023390</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992937</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.026567</td>\n",
       "      <td>0.996347</td>\n",
       "      <td>0.990913</td>\n",
       "      <td>0.985986</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.018202</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994523</td>\n",
       "      <td>0.994069</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.024149</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.995879</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.015262</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993569</td>\n",
       "      <td>0.999538</td>\n",
       "      <td>0.987671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.020945</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>0.991788</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.989545</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993374</td>\n",
       "      <td>0.994056</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>0.991360</td>\n",
       "      <td>0.987319</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.016220</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994053</td>\n",
       "      <td>0.995875</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.024325</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>0.991364</td>\n",
       "      <td>0.986878</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.018108</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.994061</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.019447</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993831</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.994511</td>\n",
       "      <td>0.996334</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.990446</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994050</td>\n",
       "      <td>0.996330</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993817</td>\n",
       "      <td>0.996785</td>\n",
       "      <td>0.990868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.030254</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.991581</td>\n",
       "      <td>0.988209</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.024791</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993390</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993390</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.025565</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993390</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.019290</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994072</td>\n",
       "      <td>0.992714</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.022940</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993840</td>\n",
       "      <td>0.993160</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994067</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.018346</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994061</td>\n",
       "      <td>0.994516</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.027265</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>0.992026</td>\n",
       "      <td>0.989995</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.035565</td>\n",
       "      <td>0.994703</td>\n",
       "      <td>0.986890</td>\n",
       "      <td>0.977171</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993122</td>\n",
       "      <td>0.997238</td>\n",
       "      <td>0.989041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.020610</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.991570</td>\n",
       "      <td>0.989541</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.021214</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992474</td>\n",
       "      <td>0.991344</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>0.990238</td>\n",
       "      <td>0.984650</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.025998</td>\n",
       "      <td>0.996164</td>\n",
       "      <td>0.990459</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>0.996347</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>0.986425</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.036337</td>\n",
       "      <td>0.994612</td>\n",
       "      <td>0.986661</td>\n",
       "      <td>0.977161</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.026188</td>\n",
       "      <td>0.996164</td>\n",
       "      <td>0.990468</td>\n",
       "      <td>0.984657</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.026308</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992892</td>\n",
       "      <td>0.997236</td>\n",
       "      <td>0.988584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.020863</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994048</td>\n",
       "      <td>0.996786</td>\n",
       "      <td>0.991324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.022577</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993157</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.032909</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>0.990243</td>\n",
       "      <td>0.984213</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.990455</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.024953</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>0.991993</td>\n",
       "      <td>0.994039</td>\n",
       "      <td>0.989954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.023306</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992940</td>\n",
       "      <td>0.990459</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.033695</td>\n",
       "      <td>0.995708</td>\n",
       "      <td>0.989340</td>\n",
       "      <td>0.982875</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.019130</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993831</td>\n",
       "      <td>0.994513</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.020812</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993843</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.016819</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993374</td>\n",
       "      <td>0.994056</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993157</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.021738</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992940</td>\n",
       "      <td>0.990459</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.024086</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992937</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.021910</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.028426</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.989101</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.989101</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.029217</td>\n",
       "      <td>0.996438</td>\n",
       "      <td>0.991138</td>\n",
       "      <td>0.986431</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.024101</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992697</td>\n",
       "      <td>0.992245</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992697</td>\n",
       "      <td>0.992245</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.029926</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>0.990234</td>\n",
       "      <td>0.985088</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.034205</td>\n",
       "      <td>0.996164</td>\n",
       "      <td>0.990463</td>\n",
       "      <td>0.985095</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.990455</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>0.995160</td>\n",
       "      <td>0.988001</td>\n",
       "      <td>0.979793</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.995342</td>\n",
       "      <td>0.988448</td>\n",
       "      <td>0.980674</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.025053</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992478</td>\n",
       "      <td>0.990897</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.023584</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993157</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992930</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.024105</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993157</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.024159</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992930</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993157</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.022913</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992930</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.025520</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.025493</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.990455</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.024932</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.990455</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.045507</td>\n",
       "      <td>0.994886</td>\n",
       "      <td>0.987325</td>\n",
       "      <td>0.978905</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.039088</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989653</td>\n",
       "      <td>0.996758</td>\n",
       "      <td>0.982648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993616</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.022433</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994072</td>\n",
       "      <td>0.992714</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993625</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>0.992044</td>\n",
       "      <td>0.987777</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.030797</td>\n",
       "      <td>0.996347</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.985547</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993619</td>\n",
       "      <td>0.991811</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992717</td>\n",
       "      <td>0.989564</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993166</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992262</td>\n",
       "      <td>0.989111</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.991589</td>\n",
       "      <td>0.987325</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026029</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>0.992040</td>\n",
       "      <td>0.988219</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.033606</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.034034</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.033391</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.033318</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.033123</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.033010</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.031990</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.031977</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.031828</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.985102</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>0.996347</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.985547</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.024832</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989761</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.021456</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992461</td>\n",
       "      <td>0.993141</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993144</td>\n",
       "      <td>0.994053</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.991585</td>\n",
       "      <td>0.987766</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.012018</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.993604</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.017649</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993387</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016077</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994291</td>\n",
       "      <td>0.994518</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.027260</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.991578</td>\n",
       "      <td>0.988652</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026635</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>0.990234</td>\n",
       "      <td>0.985088</td>\n",
       "      <td>0.995434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.034771</td>\n",
       "      <td>0.995251</td>\n",
       "      <td>0.988225</td>\n",
       "      <td>0.980234</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.032409</td>\n",
       "      <td>0.995342</td>\n",
       "      <td>0.988448</td>\n",
       "      <td>0.980674</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.029292</td>\n",
       "      <td>0.995525</td>\n",
       "      <td>0.988891</td>\n",
       "      <td>0.981990</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.031149</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>0.981548</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.053138</td>\n",
       "      <td>0.992877</td>\n",
       "      <td>0.982448</td>\n",
       "      <td>0.968500</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.036902</td>\n",
       "      <td>0.995525</td>\n",
       "      <td>0.988891</td>\n",
       "      <td>0.981990</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.031983</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>0.990238</td>\n",
       "      <td>0.984650</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>0.990238</td>\n",
       "      <td>0.984650</td>\n",
       "      <td>0.995890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.037676</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.988678</td>\n",
       "      <td>0.980683</td>\n",
       "      <td>0.996804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.036353</td>\n",
       "      <td>0.995616</td>\n",
       "      <td>0.989121</td>\n",
       "      <td>0.981998</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.036806</td>\n",
       "      <td>0.995616</td>\n",
       "      <td>0.989121</td>\n",
       "      <td>0.981998</td>\n",
       "      <td>0.996347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.033205</td>\n",
       "      <td>0.993790</td>\n",
       "      <td>0.984678</td>\n",
       "      <td>0.971975</td>\n",
       "      <td>0.997717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.018493</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.988657</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>0.992033</td>\n",
       "      <td>0.989106</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993387</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993837</td>\n",
       "      <td>0.993610</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.023175</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.988657</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.015138</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993834</td>\n",
       "      <td>0.994061</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016701</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993387</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.018409</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.016546</td>\n",
       "      <td>0.997534</td>\n",
       "      <td>0.993829</td>\n",
       "      <td>0.994966</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.017634</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992921</td>\n",
       "      <td>0.993148</td>\n",
       "      <td>0.992694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.020465</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993160</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.992934</td>\n",
       "      <td>0.991352</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>0.996332</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.019396</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994053</td>\n",
       "      <td>0.995875</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.018595</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994053</td>\n",
       "      <td>0.995875</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.024018</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.991348</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.022164</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992711</td>\n",
       "      <td>0.990455</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.017749</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.017914</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.992704</td>\n",
       "      <td>0.994064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.994061</td>\n",
       "      <td>0.994516</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.017032</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993607</td>\n",
       "      <td>0.993607</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.017067</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.994286</td>\n",
       "      <td>0.995423</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.993604</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.993604</td>\n",
       "      <td>0.994059</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.019709</td>\n",
       "      <td>0.997352</td>\n",
       "      <td>0.993381</td>\n",
       "      <td>0.993154</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.021756</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992474</td>\n",
       "      <td>0.991344</td>\n",
       "      <td>0.993607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993160</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.022732</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.993160</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.025653</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.024791</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.029129</td>\n",
       "      <td>0.996438</td>\n",
       "      <td>0.991126</td>\n",
       "      <td>0.987755</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026558</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.028764</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>0.992033</td>\n",
       "      <td>0.989106</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>0.992029</td>\n",
       "      <td>0.989550</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.027219</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.989555</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.022560</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.024241</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>0.992707</td>\n",
       "      <td>0.990901</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.021851</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.021585</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.022911</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.989101</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.023096</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.989101</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.989101</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.989101</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.025045</td>\n",
       "      <td>0.996438</td>\n",
       "      <td>0.991126</td>\n",
       "      <td>0.987755</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>0.984636</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.027450</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>0.984636</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.027060</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.027151</td>\n",
       "      <td>0.995799</td>\n",
       "      <td>0.989550</td>\n",
       "      <td>0.984629</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.027506</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>0.984636</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.028301</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>0.984636</td>\n",
       "      <td>0.994977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>0.995799</td>\n",
       "      <td>0.989550</td>\n",
       "      <td>0.984629</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.028567</td>\n",
       "      <td>0.995799</td>\n",
       "      <td>0.989550</td>\n",
       "      <td>0.984629</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.996164</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>0.986413</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026848</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990675</td>\n",
       "      <td>0.986860</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.026883</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990675</td>\n",
       "      <td>0.986860</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.026890</td>\n",
       "      <td>0.996256</td>\n",
       "      <td>0.990675</td>\n",
       "      <td>0.986860</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10950\n",
      "  Batch size = 256\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100000, training_loss=0.0014249664500355721, metrics={'train_runtime': 54256.8643, 'train_samples_per_second': 471.83, 'train_steps_per_second': 1.843, 'total_flos': 1.6915737094438748e+18, 'train_loss': 0.0014249664500355721, 'epoch': 291.55})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c52d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - tf",
   "language": "python",
   "name": "python3-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
